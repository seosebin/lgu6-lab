{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 방법 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "업로드 중...\n",
      "업로드 완료!\n",
      "284807 rows loaded to lgu6h-project.sample_data.creditcard_fraud\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import os\n",
    "\n",
    "# 서비스 계정 키 경로 설정\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/Admin/Desktop/de_basic/gcp_tutorial/lgu6h-project-09e19171e4dd.json\"\n",
    "\n",
    "# BigQuery 클라이언트 초기화\n",
    "client = bigquery.Client()\n",
    "\n",
    "# BigQuery 테이블 참조 정보 정의\n",
    "project_id = \"lgu6h-project\"\n",
    "dataset_id = \"sample_data\"\n",
    "table_id = \"creditcard_fraud\"\n",
    "table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "# 로컬 CSV 파일 경로 지정\n",
    "file_path = \"C:/Users/Admin/Desktop/de_basic/gcp_tutorial/creditcard.csv\"\n",
    "\n",
    "# BigQuery 테이블 스키마 정의\n",
    "# Time 컬럼: FLOAT64 타입\n",
    "# V1~V28 컬럼: FLOAT64 타입 (특성 변수들)\n",
    "# Amount 컬럼: FLOAT64 타입 (거래 금액)\n",
    "# Class 컬럼: INTEGER 타입 (사기 여부: 0=정상, 1=사기)\n",
    "schema = [bigquery.SchemaField(\"Time\", \"FLOAT64\")] + \\\n",
    "         [bigquery.SchemaField(f\"V{i}\", \"FLOAT64\") for i in range(1, 29)] + [\n",
    "             bigquery.SchemaField(\"Amount\", \"FLOAT64\"),\n",
    "             bigquery.SchemaField(\"Class\", \"INTEGER\"),\n",
    "         ]\n",
    "\n",
    "# BigQuery 로드 작업 설정\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.CSV,  # 소스 파일 형식: CSV\n",
    "    skip_leading_rows=1,                      # 헤더 행 건너뛰기\n",
    "    schema=schema,                            # 미리 정의된 스키마 사용\n",
    "    write_disposition=\"WRITE_TRUNCATE\"        # 기존 테이블 덮어쓰기\n",
    ")\n",
    "\n",
    "# CSV 파일을 BigQuery로 업로드\n",
    "with open(file_path, \"rb\") as source_file:\n",
    "    load_job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "\n",
    "# 업로드 진행 상황 출력\n",
    "print(\"업로드 중...\")\n",
    "load_job.result()  # 업로드 완료까지 대기\n",
    "print(\"업로드 완료!\")\n",
    "\n",
    "# 업로드 결과 확인 및 출력\n",
    "table = client.get_table(table_ref)\n",
    "print(f\"{table.num_rows} rows loaded to {table.project}.{table.dataset_id}.{table.table_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 방법 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284807 rows uploaded to lgu6h-project.sample_data.creditcard_fraud\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import pandas as pd  # 데이터 조작 및 분석을 위한 라이브러리\n",
    "from google.cloud import bigquery  # BigQuery 클라이언트 라이브러리\n",
    "import os  # 운영체제 관련 기능 (환경변수 설정 등)\n",
    "\n",
    "# Google Cloud 서비스 계정 인증 설정\n",
    "# 서비스 계정 키 파일 경로를 환경변수로 설정하여 BigQuery 접근 권한 획득\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/Admin/Desktop/de_basic/gcp_tutorial/lgu6h-project-09e19171e4dd.json\"\n",
    "\n",
    "# 1단계: Pandas를 사용하여 로컬 CSV 파일을 DataFrame으로 로드\n",
    "# read_csv() 함수로 CSV 파일을 읽어와서 메모리에 DataFrame 형태로 저장\n",
    "df = pd.read_csv(\"C:/Users/Admin/Desktop/de_basic/gcp_tutorial/creditcard.csv\")\n",
    "\n",
    "# 2단계: BigQuery 클라이언트 객체 생성\n",
    "# 이 클라이언트를 통해 BigQuery 서비스와 통신하여 데이터 업로드/다운로드 수행\n",
    "client = bigquery.Client()\n",
    "\n",
    "# 3단계: BigQuery 테이블 정보 정의\n",
    "project_id = \"lgu6h-project\"      # Google Cloud 프로젝트 ID\n",
    "dataset_id = \"sample_data\"        # BigQuery 데이터셋 ID\n",
    "table_id = \"creditcard_fraud\"     # BigQuery 테이블 ID\n",
    "# 전체 테이블 식별자 생성 (project.dataset.table 형식)\n",
    "full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "# 4단계: BigQuery 업로드 작업 설정\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # 기존 테이블이 있으면 완전히 덮어쓰기\n",
    "    autodetect=True                      # 스키마 자동 감지 (컬럼 타입 자동 추론)\n",
    ")\n",
    "\n",
    "# 5단계: Pandas DataFrame을 BigQuery 테이블로 업로드\n",
    "# load_table_from_dataframe() 메서드로 DataFrame을 BigQuery에 전송\n",
    "job = client.load_table_from_dataframe(df, full_table_id, job_config=job_config)\n",
    "job.result()  # 업로드 작업이 완료될 때까지 대기 (동기 실행)\n",
    "\n",
    "# 6단계: 업로드 완료 결과 출력\n",
    "# DataFrame의 행 개수와 업로드된 테이블 정보를 출력하여 성공 여부 확인\n",
    "print(f\"{df.shape[0]} rows uploaded to {full_table_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 방법 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_23740\\3171931068.py:20: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  df.to_gbq(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "업로드 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_23740\\3171931068.py:30: FutureWarning: read_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.read_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.read_gbq\n",
      "  df_from_bq = pd.read_gbq(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불러온 행 개수: 284807\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import pandas as pd  # 데이터 조작 및 분석을 위한 라이브러리\n",
    "import os  # 운영체제 관련 기능 (환경변수 설정 등)\n",
    "\n",
    "# Google Cloud 서비스 계정 인증 설정\n",
    "# 서비스 계정 키 파일 경로를 환경변수로 설정하여 BigQuery 접근 권한 획득\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/Admin/Desktop/de_basic/gcp_tutorial/lgu6h-project-09e19171e4dd.json\"\n",
    "\n",
    "# 1단계: Pandas를 사용하여 로컬 CSV 파일을 DataFrame으로 로드\n",
    "# read_csv() 함수로 CSV 파일을 읽어와서 메모리에 DataFrame 형태로 저장\n",
    "df = pd.read_csv(\"C:/Users/Admin/Desktop/de_basic/gcp_tutorial/creditcard.csv\")\n",
    "\n",
    "# 2단계: BigQuery 테이블 정보 정의\n",
    "table_id = \"sample_data.creditcard_fraud\"  # 데이터셋.테이블명 형식 (프로젝트는 별도 지정)\n",
    "project_id = \"lgu6h-project\"               # Google Cloud 프로젝트 ID\n",
    "\n",
    "# 3단계: Pandas DataFrame을 BigQuery 테이블로 업로드\n",
    "# to_gbq() 메서드를 사용하여 DataFrame을 BigQuery에 직접 전송\n",
    "# if_exists=\"replace\": 기존 테이블이 있으면 완전히 덮어쓰기\n",
    "df.to_gbq(\n",
    "    destination_table=table_id,  # 대상 테이블 식별자\n",
    "    project_id=project_id,       # 프로젝트 ID\n",
    "    if_exists=\"replace\"          # 기존 테이블 처리 방식 (덮어쓰기)\n",
    ")\n",
    "\n",
    "print(\"업로드 완료!\")\n",
    "\n",
    "# 4단계: BigQuery에서 데이터를 다시 가져오기 (검증 목적)\n",
    "# read_gbq() 메서드를 사용하여 SQL 쿼리로 데이터 조회\n",
    "df_from_bq = pd.read_gbq(\n",
    "    query=\"SELECT * FROM `lgu6h-project.sample_data.creditcard_fraud`\",  # 전체 데이터 조회 쿼리\n",
    "    project_id=project_id,  # 프로젝트 ID\n",
    "    dialect=\"standard\"      # BigQuery SQL 방언 사용\n",
    ")\n",
    "\n",
    "# 5단계: 업로드 결과 검증\n",
    "# DataFrame의 행 개수를 출력하여 성공적으로 데이터가 업로드되었는지 확인\n",
    "print(f\"불러온 행 개수: {len(df_from_bq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 쿼리 통해서 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV 로드 완료: 284807 rows (소요: 0:00:00)\n",
      "📤 BigQuery 업로드 중...\n",
      "✅ 업로드 완료 (소요: 0:00:08)\n",
      "🔍 쿼리 완료 (소요: 0:00:00)\n",
      "상위 5행:\n",
      "    Time        V1        V2        V3        V4        V5        V6  \\\n",
      "0  282.0 -0.356466  0.725418  1.971749  0.831343  0.369681 -0.107776   \n",
      "1  380.0 -1.299837  0.881817  1.452842 -1.293698 -0.025105 -1.170103   \n",
      "2  403.0  1.237413  0.512365  0.687746  1.693872 -0.236323 -0.650232   \n",
      "3  430.0 -1.860258 -0.629859  0.966570  0.844632  0.759983 -1.481173   \n",
      "4  711.0 -0.431349  1.027694  2.670816  2.084787 -0.274567  0.286856   \n",
      "\n",
      "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
      "0  0.751610 -0.120166 -0.420675  ...  0.020804  0.424312 -0.015989  0.466754   \n",
      "1  0.861610 -0.193934  0.592001  ... -0.272563 -0.360853  0.223911  0.598930   \n",
      "2  0.118066 -0.230545 -0.808523  ... -0.077543 -0.178220  0.038722  0.471218   \n",
      "3 -0.509681  0.540722 -0.733623  ...  0.268028  0.125515 -0.225029  0.586664   \n",
      "4  0.152110  0.200872 -0.596505  ...  0.001241  0.154170 -0.141533  0.384610   \n",
      "\n",
      "        V25       V26       V27       V28  Amount  Class  \n",
      "0 -0.809962  0.657334 -0.043150 -0.046401     0.0      0  \n",
      "1 -0.397705  0.637141  0.234872  0.021379     0.0      0  \n",
      "2  0.289249  0.871803 -0.066884  0.012986     0.0      0  \n",
      "3 -0.031598  0.570168 -0.043007 -0.223739     0.0      0  \n",
      "4 -0.147132 -0.087100  0.101117  0.077944     0.0      0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "📊 BigQuery에서 불러온 총 행 수: 10\n",
      "⏱️ 전체 소요 시간: 0:00:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\de_basic\\gcp_tutorial\\.venv\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:1957: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import pandas as pd  # 데이터 조작 및 분석을 위한 라이브러리\n",
    "from google.cloud import bigquery  # Google BigQuery 클라이언트 라이브러리\n",
    "import os  # 운영체제 관련 기능 (환경변수 설정 등)\n",
    "import time  # 시간 측정을 위한 라이브러리\n",
    "from datetime import timedelta  # 시간 간격 계산을 위한 라이브러리\n",
    "\n",
    "# 시간 포맷 함수 정의\n",
    "# 초 단위 시간을 시:분:초 형태의 문자열로 변환하는 함수\n",
    "def format_time(seconds):\n",
    "    return str(timedelta(seconds=int(seconds)))\n",
    "\n",
    "# 전체 프로세스 시작 시간 기록\n",
    "# 성능 측정을 위해 전체 실행 시간을 추적하기 위한 시작점 설정\n",
    "total_start = time.time()\n",
    "\n",
    "# 0. Google Cloud 서비스 계정 인증 설정\n",
    "# 서비스 계정 키 파일 경로를 환경변수로 설정하여 BigQuery 접근 권한 획득\n",
    "# 이 파일에는 Google Cloud 프로젝트에 대한 인증 정보가 포함되어 있음\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/Admin/Desktop/de_basic/gcp_tutorial/lgu6h-project-09e19171e4dd.json\"\n",
    "\n",
    "# 1. 로컬 CSV 파일을 DataFrame으로 로드\n",
    "# read_csv() 함수를 사용하여 CSV 파일을 메모리에 DataFrame 형태로 읽어옴\n",
    "start = time.time()  # CSV 로드 시작 시간 기록\n",
    "df = pd.read_csv(\"C:/Users/Admin/Desktop/de_basic/gcp_tutorial/creditcard.csv\")\n",
    "print(f\"✅ CSV 로드 완료: {df.shape[0]} rows (소요: {format_time(time.time() - start)})\")\n",
    "\n",
    "# 2. BigQuery 클라이언트 객체 생성\n",
    "# Google Cloud BigQuery 서비스와 통신하기 위한 클라이언트 인스턴스 생성\n",
    "# 이 클라이언트를 통해 데이터 업로드, 쿼리 실행 등의 작업 수행\n",
    "client = bigquery.Client()\n",
    "\n",
    "# 3. BigQuery 테이블 정보 정의\n",
    "# 프로젝트, 데이터셋, 테이블 ID를 개별적으로 정의하고 전체 테이블 식별자 생성\n",
    "project_id = \"lgu6h-project\"    # Google Cloud 프로젝트 ID\n",
    "dataset_id = \"sample_data\"      # BigQuery 데이터셋 ID\n",
    "table_id = \"creditcard_fraud\"   # BigQuery 테이블 ID\n",
    "full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"  # 전체 테이블 식별자 (프로젝트.데이터셋.테이블)\n",
    "\n",
    "# 4. BigQuery 업로드 작업 설정\n",
    "# LoadJobConfig 객체를 생성하여 데이터 업로드 시의 동작 방식 정의\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # 기존 테이블이 있으면 완전히 덮어쓰기\n",
    "    autodetect=True                      # 스키마 자동 감지 (컬럼 타입 자동 설정)\n",
    ")\n",
    "\n",
    "# 5. DataFrame을 BigQuery 테이블로 업로드\n",
    "# load_table_from_dataframe() 메서드를 사용하여 Pandas DataFrame을 BigQuery에 전송\n",
    "start = time.time()  # 업로드 시작 시간 기록\n",
    "print(\"📤 BigQuery 업로드 중...\")\n",
    "job = client.load_table_from_dataframe(df, full_table_id, job_config=job_config)  # 비동기 업로드 작업 시작\n",
    "job.result()  # 업로드 작업이 완료될 때까지 대기 (동기화)\n",
    "print(f\"✅ 업로드 완료 (소요: {format_time(time.time() - start)})\")\n",
    "\n",
    "# 6. BigQuery에서 데이터 조회 (검증 목적)\n",
    "# SQL 쿼리를 실행하여 업로드된 데이터를 다시 가져와서 검증\n",
    "start = time.time()  # 쿼리 시작 시간 기록\n",
    "query = f\"SELECT * FROM `{full_table_id}` LIMIT 10\"  # 전체 데이터에서 상위 10행만 조회하는 SQL 쿼리\n",
    "df_from_bq = client.query(query).to_dataframe()  # 쿼리 실행 결과를 DataFrame으로 변환\n",
    "print(f\"🔍 쿼리 완료 (소요: {format_time(time.time() - start)})\")\n",
    "\n",
    "# 7. 결과 데이터 출력 및 검증\n",
    "# 업로드된 데이터의 일부를 출력하여 성공적으로 처리되었는지 확인\n",
    "print(\"상위 5행:\")\n",
    "print(df_from_bq.head())  # DataFrame의 처음 5행 출력\n",
    "print(f\"📊 BigQuery에서 불러온 총 행 수: {len(df_from_bq)}\")  # 조회된 총 행 수 출력\n",
    "\n",
    "# 전체 프로세스 소요 시간 출력\n",
    "# CSV 로드부터 BigQuery 쿼리까지 전체 과정에 걸린 시간을 계산하여 출력\n",
    "print(f\"⏱️ 전체 소요 시간: {format_time(time.time() - total_start)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
