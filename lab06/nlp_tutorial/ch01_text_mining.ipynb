{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4Nznozcr3Mm"
   },
   "source": [
    "## I. 개요\n",
    "- NLP(Natural Language Processing): 기계가 인간의 언어를 이해하고 해석하는 데 중점\n",
    "  + 활용예제: 기계 번역, 챗봇, 질의응답 시스템 (딥러닝)\n",
    "- Text Analysis: 비정형 텍스트에서 의미 있는 정보를 추출하는 것에 중점\n",
    "  + 활용예제: 비즈니스 인텔리전스, 예측분석 (머신러닝)\n",
    "- 텍스트 분석의 예\n",
    "  + 텍스트 분류: 문서가 특정 분류 또는 카테고리에 속하는 것을 예측하는 기법\n",
    "  + 감성 분석: 텍스트에서 나타나는 감정/판단/믿음/의견 등의 주관적인 요소 분석하는 기법\n",
    "  + 텍스트 요약: 텍스트 내에서의 중요한 주제나 중심 사상 추출(Topic Modeling)\n",
    "  + 텍스트 군집화(Clustering)와 유사도 측정: 비슷한 유형의 문서에 대해 군집화를 수행하는 기법. 텍스트 분류를 비지도학습으로 수행하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXH37qLYvz94"
   },
   "source": [
    "## II. 텍스트 분석 개요\n",
    "- 텍스트를 의미있는 숫자로 표현하는 것이 핵심\n",
    "- 영어 키워드: Feature Vectorization 또는 Feature Extraction. \n",
    "- 텍스트를 Feature Vectorization에는 BOW(Bag of Words)와 Word2Vec 두가지 방법이 존재. \n",
    "- 머신러닝을 수행하기 전에 반드시 선행되어야 함. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPKRkmdDx475"
   },
   "source": [
    "### (1) 텍스트 분석 수행 방법\n",
    "- 1단계: 데이터 전처리 수행. 클렌징, 대/소문자 변경, 특수문자 삭제. 단어 등의 토큰화 작업, 의미 없는 단어(Stop word) 제거 작업, 어근 추출(Stemming/Lemmdatization)등의 텍스트 정규화 작업 필요\n",
    "- 2단계: 피처 벡터화/추출: 가공된 텍스트에서 피처 추출 및 벡터 값 할당. \n",
    "  + Bag of Words: Count 기반 or TF-IDF 기반 벡터화\n",
    "- 3단계: ML 모델 수립 및 학습/예측/평가를 수행. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ps4X-tYdVIp_"
   },
   "source": [
    "### (2) 파이썬 기반의 NLP, 텍스트 분석 패키지\n",
    "- `NTLK`: 파이썬의 가장 대표적인 NLP 패키지. 방대한 데이터 세트와 서브 모듈 보유. 그러나, 속도가 느리다는 단점 존재\n",
    "  + Docs: https://www.nltk.org/\n",
    "- 'Gensim': 토픽 모델링 분야에서 주로 사용되는 패키지. Word2Vec 구현도 가능\n",
    "  + Docs: https://radimrehurek.com/gensim/\n",
    "- `SpaCY`: 최근 가장 주목을 받는 `NLP` 패키지. \n",
    "  + Docs: https://spacy.io/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VopCDXyAWAwQ"
   },
   "source": [
    "## III. 텍스트 전처리 - 정규화\n",
    "- 텍스트 자체를 바로 피처로 만들 수는 없다. 텍스트를 가공하기 위한 클렌징, 토큰화, 어근화 등이 필요. \n",
    "- 정규화 작업의 종류는 다음과 같음\n",
    "  + 클렌징: 불필요한 문자,기호 등을 사전제거 (정규표현식 주로 활용)\n",
    "  + 토큰화\n",
    "  + 필터링/스톱 워드 제거/철자 수정\n",
    "  + Stemming\n",
    "  + Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lP2s2XrDXHI-"
   },
   "source": [
    "### (1) 문장 토큰화\n",
    "- 문장 토큰화(sentence tokenization)는 문장의 마침표, 개행문자(\\n) 등 문장의 마지막을 뜻하는 기호에 따라 분리하는 것이 일반적임\n",
    "- 아래 샘플코드는 문장 토큰화에 관한 것임\n",
    "- `punkt`는 마침표, 개행 문자 등의 데이터 세트를 다운로드 받는다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3231,
     "status": "ok",
     "timestamp": 1673855943099,
     "user": {
      "displayName": "Ji-hoon Jung",
      "userId": "03169308685755834042"
     },
     "user_tz": -540
    },
    "id": "SpNcm-98moRn",
    "outputId": "63dc450c-6701-4b1f-f22a-546d5b81f555"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1673855943100,
     "user": {
      "displayName": "Ji-hoon Jung",
      "userId": "03169308685755834042"
     },
     "user_tz": -540
    },
    "id": "Dx079kF9Xoab",
    "outputId": "253799cb-b4b3-4d2e-f047-00ed353e8499"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this wroom.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "text_sample = \"The Matrix is everywhere its all around us, here even in this wroom. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.\"\n",
    "\n",
    "sentences = sent_tokenize(text = text_sample)\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T82hUf7wYIfa"
   },
   "source": [
    "- 위 코드에서 확인할 수 있는 것은 `sent_tokenize`가 반환하는 것은 각각의 문장으로 구성된 list 객체이며, 이 객체는 3개의 문장으로 된 문자열을 가지고 있음을 알 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVD95bZsYZ4C"
   },
   "source": [
    "### (2) 단어 토큰화\n",
    "- 단어 토큰화(Word Tokenization)는 문장을 단어로 토큰화하는 것을 말하며, 기본적으로 공백, 콤마(,), 마침표(.), 개행문자 등으로 단어를 분리하지만, 정규 표현식을 이용해 다양한 유형으로 토큰화를 수행할 수 있다. \n",
    "- 단어의 순서가 중요하지 않은 경우에는 Bag of Word를 사용해도 된다. \n",
    "- 이제 코드를 구현해본다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1673855943100,
     "user": {
      "displayName": "Ji-hoon Jung",
      "userId": "03169308685755834042"
     },
     "user_tz": -540
    },
    "id": "kZ7wCnibY-JL",
    "outputId": "307cbe76-d15a-4909-e585-240937ad775a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D3yZVtVZU0_"
   },
   "source": [
    "- 이번에는 문장 및 단어 토큰화를 함수로 구현해보도록 한다. \n",
    "  + 우선, 문장별로 토큰을 분리한 후\n",
    "  + 분리된 문장별 단어를 토큰화로 진행하는 코드를 구현한다 (for loop 활용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1673855943101,
     "user": {
      "displayName": "Ji-hoon Jung",
      "userId": "03169308685755834042"
     },
     "user_tz": -540
    },
    "id": "xTDmSZjkQl5v",
    "outputId": "6c1af95e-ea4e-434f-9beb-4719d554939b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'wroom', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# 여러 개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화하게 만드는 함수\n",
    "def tokenize_text(text):\n",
    "\n",
    "  # 문장별로 분리 토큰\n",
    "  sentences = sent_tokenize(text)\n",
    "\n",
    "  # 분리된 문장별 단어 토큰화\n",
    "  word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "  return word_tokens\n",
    "\n",
    "# 여러 문장에 대해 문장별 단어 토큰화 수행\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbzccp9aSzfx"
   },
   "source": [
    "- 각각의 개별 리스트는 해당 문장에 대한 토큰화된 단어를 요소로 가진다. \n",
    "- 문장을 단어별로 하나씩 토큰화 할 경우 문맥적인 의미는 무시될 수 밖에 없는데.. 이러한 문제를 해결하기 위해 도입된 개념이 n-gram이다. \n",
    "- `N-gram`은 연속된 N개의 단어를 하나의 토큰화 단위로 분리해 내는 것. \n",
    "  + 예시) I Love You \n",
    "  + (I, Love), (Love, You)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nM4FD0CDGH9"
   },
   "source": [
    "## IV. 텍스트 전처리 - 스톱 워드(불용어) 제거\n",
    "- 의미가 없는 `be`동사 등을 제거 할 때 사용함\n",
    "  + 이런 단어들은 매우 자주 나타나는 특징이 있음\n",
    "- `NTLK`의 스톱 워드에 기본적인 세팅이 저장되어 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1673855943101,
     "user": {
      "displayName": "Ji-hoon Jung",
      "userId": "03169308685755834042"
     },
     "user_tz": -540
    },
    "id": "XUctup-TFywe",
    "outputId": "bd4c45ef-796d-4263-dd98-2de7d131a3eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e77yzSPGmDU"
   },
   "source": [
    "- 총 몇개의 `stopwords`가 있는지 알아보고, 그중 20개만 확인해본다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1673855943101,
     "user": {
      "displayName": "Ji-hoon Jung",
      "userId": "03169308685755834042"
     },
     "user_tz": -540
    },
    "id": "4yU2vi9TGk1_",
    "outputId": "21683b2d-fac4-4168-fa81-10988cc0b357"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 개수: 198\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n"
     ]
    }
   ],
   "source": [
    "print(\"영어 stop words 개수:\", len(nltk.corpus.stopwords.words(\"english\")))\n",
    "print(nltk.corpus.stopwords.words(\"english\")[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seJhsh5aHGs4"
   },
   "source": [
    "- 이번에는 `stopwords`를 필터링으로 제거하여 분석을 위한 의미 있는 단어만 추출하도록 함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1673855943102,
     "user": {
      "displayName": "Ji-hoon Jung",
      "userId": "03169308685755834042"
     },
     "user_tz": -540
    },
    "id": "exfnpiB2HEtc",
    "outputId": "d2e08f45-3fa9-47dd-b689-8967a44eeae7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'wroom', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "all_tokens = []\n",
    "\n",
    "# 위 예제에서 3개의 문장별로 얻은 word_tokens list에 대해 불용어 제거하는 반복문 작성\n",
    "for sentence in word_tokens:\n",
    "  filtered_words = [] # 빈 리스트 생성\n",
    "\n",
    "  # 개별 문장별로 토큰화된 문장 list에 대해 스톱 워드 제거\n",
    "  for word in sentence:\n",
    "\n",
    "    # 소문자로 모두 변환\n",
    "    word = word.lower()\n",
    "\n",
    "    # 토큰화된 개별 단어가 스톱 워드의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "    if word not in stopwords:\n",
    "      filtered_words.append(word)\n",
    "  all_tokens.append(filtered_words)\n",
    "  \n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLjo4YiaJSNR"
   },
   "source": [
    "- `is`, `this`와 같은 불용어가 처리된 것 확인됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3fGJY7sQpDU"
   },
   "source": [
    "## V. 텍스트 전처리 - 어간(Stemming) 및 표제어(Lemmatization)\n",
    "- 동사의 변화\n",
    "  + 예) Love, Loved, Loving\n",
    "- 어근 및 표제어는 단어의 원형을 찾는 것. \n",
    "- 그런데, 표제어 추출(Lemmatization)이 어근(Stemming)보다는 보다 더 의미론적인 기반에서 단어의 원형을 찾는다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jl7n9LZtSKTZ"
   },
   "source": [
    "### (1) 어간(Stemming)\n",
    "- `Stemming`은 원형 단어로 변환 시, 어미를 제거하는 방식을 사용한다. \n",
    "  + 예) `worked`에서 `ed`를 제거하는 방식을 사용\n",
    "- `Stemming`기법에는 크게 `Porter`, `Lancaster`, `Snowball Stemmer`가 있음. \n",
    "- 소스코드 예시는 아래와 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1673855943102,
     "user": {
      "displayName": "Ji-hoon Jung",
      "userId": "03169308685755834042"
     },
     "user_tz": -540
    },
    "id": "h38nY2tgRRpu",
    "outputId": "e33bb0fe-a508-427f-8e22-a7138355d876"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      lancaster Stemmer   \n",
      "friend              friend              friend              \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "stabil              stabil              stabl               \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,porter.stem(word),lancaster.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWeafwS3vdI4"
   },
   "source": [
    "- LancasterStemmer 간단하지만, 가끔 지나치게 over-stemming을 하는 경향이 있다. 이는 문맥적으로는 큰 의미가 없을수도 있기 때문에 주의를 요망한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1673855943103,
     "user": {
      "displayName": "Ji-hoon Jung",
      "userId": "03169308685755834042"
     },
     "user_tz": -540
    },
    "id": "WU-zMg0OwiR-",
    "outputId": "8e8b11a8-8cbc-4137-d080-33010d155592"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Lancaster: dest\n",
      "For Porter: destabil\n"
     ]
    }
   ],
   "source": [
    "print(\"For Lancaster:\", lancaster.stem(\"destabilized\"))\n",
    "print(\"For Porter:\", porter.stem(\"destabilized\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNzIdV3Sw1oE"
   },
   "source": [
    "- 위와 같이 `destabilized(불안정한)` 뜻을 가진 단어가 `destabil(불안정)`이 아닌 `dest(목적지)`로 변환되기도 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmX3LJ3ExpSn"
   },
   "source": [
    "### (2) 표제어 추출(Lemmatization)\n",
    "- 표제어 추출은 품사와 같은 문법적인 요소와 더 의미적인 부분을 감안하여 정확한 철자로 된 어근 단어를 찾아준다. \n",
    "- 어근을 보통 `Lemma`라고 부르며, 이 때의 어근은 Canoical Form, Dictionary Form, Citation Form 이라고 부른다. \n",
    "- 간단하게 예를 들면, `loves`, `loving`, `loved`는 모두 `love`에서 파생된 것이며, 이 때 `love`는 `Lemma`라고 부른다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 626,
     "status": "ok",
     "timestamp": 1673856011225,
     "user": {
      "displayName": "Ji-hoon Jung",
      "userId": "03169308685755834042"
     },
     "user_tz": -540
    },
    "id": "HE9rNoJJxLFy",
    "outputId": "ee6bff58-e7a0-475c-bde9-20e74e19605e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xn3sTQ0WzzOs"
   },
   "source": [
    "- 간단하게 단어들을 확인해본다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2128,
     "status": "ok",
     "timestamp": 1673856019432,
     "user": {
      "displayName": "Ji-hoon Jung",
      "userId": "03169308685755834042"
     },
     "user_tz": -540
    },
    "id": "zNtT1TqjzZgL",
    "outputId": "8b464728-b3dc-4c36-f8a9-0839cacbcfa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happier happiest\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\n",
    "print(lemma.lemmatize('happier', 'v'), lemma.lemmatize('happiest', 'v'))\n",
    "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36QImjou5sbm"
   },
   "source": [
    "- 이번에는 조금 긴 문장을 활용하여 작성하도록 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1673856023087,
     "user": {
      "displayName": "Ji-hoon Jung",
      "userId": "03169308685755834042"
     },
     "user_tz": -540
    },
    "id": "NxixDyl45uZz",
    "outputId": "cbb8f7cf-d975-4b94-d7ba-551772156306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\" # 해당되는 부호는 제외하는 코드를 만든다. \n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpwOLitS59yB"
   },
   "source": [
    "- 지금까지 진행한 것은 텍스트 전처리의 일환으로 활용한 것이다. 각각의 정규화, 불용어, 어간 및 표제어 등은 각각 함수로 작성하는 것을 권한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8p4pD8fJ8I5_"
   },
   "source": [
    "## VI. Reference\n",
    "- 권철민. (2020). 파이썬 머신러닝 완벽가이드. 경기, 파주: 위키북스\n",
    "- Jabeen, H. (2018). Stemming and Lemmatization in Python. Retreived from https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOIcWngHJTUE98yXE9id2vV",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
